{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 10:01:25.935749: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-06 10:01:26.548803: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "from functools import partial\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.utils import _term_move_up\n",
    "\n",
    "prefix = _term_move_up() + '\\r'\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# from mamba import Mamba, MambaConfig\n",
    "from mamba_ssm import Mamba\n",
    "# from mamba_ssm.modules.mamba_simple import Block\n",
    "from mamba_ssm.models.mixer_seq_simple import create_block, _init_weights\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from data import DrawingDataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce GTX 1080\n",
      "_CudaDeviceProperties(name='NVIDIA GeForce GTX 1080', major=6, minor=1, total_memory=8110MB, multi_processor_count=20)\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())\n",
    "print(torch.cuda.get_device_properties(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/345 [00:00<01:47,  3.21it/s]\n",
      "  0%|          | 1/345 [00:00<00:04, 83.16it/s]\n",
      "  0%|          | 1/345 [00:00<00:04, 70.32it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "max_length = 100\n",
    "\n",
    "train_dataset = DrawingDataset(data_path=\"./data\", split=\"train\", max_length=max_length)\n",
    "val_dataset = DrawingDataset(data_path=\"./data\", split=\"valid\", max_length=max_length)\n",
    "test_dataset = DrawingDataset(data_path=\"./data\", split=\"test\", max_length=max_length)\n",
    "\n",
    "train = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class mambaBlock(nn.Module):\n",
    "#     def __init__(self, d_model, d_state, d_conv, expand, n_layers):\n",
    "#         super(mambaBlock, self).__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.d_state = d_state\n",
    "#         self.d_conv = d_conv\n",
    "#         self.expand = expand\n",
    "#         self.n_layers = n_layers\n",
    "#         self.layers = []\n",
    "\n",
    "#         self.layers = nn.ModuleList([Mamba(d_model, d_state, d_conv, expand) for _ in range(n_layers)])\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x)\n",
    "#         return x\n",
    "\n",
    "class customModel(nn.Module):\n",
    "    def __init__(self, nb, no, ns, embed_dim, state_hidden):\n",
    "        super(customModel, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.state_hidden = state_hidden\n",
    "        self.proj = nn.Linear(in_features=5, out_features=self.embed_dim, bias=False)\n",
    "        \n",
    "        self.m1 = nn.ModuleList([create_block(self.embed_dim , device='cuda', layer_idx=f'm{i}') for i in range(nb)])\n",
    "        self.leftm = nn.ModuleList([create_block(self.embed_dim , device='cuda', layer_idx=f'l{i}') for i in range(no)])\n",
    "        self.rightm = nn.ModuleList([create_block(self.embed_dim , device='cuda', layer_idx=f'r{i}') for i in range(ns)])\n",
    "        \n",
    "        self.offset_out = nn.Linear(in_features=self.embed_dim, out_features=2, bias=False)\n",
    "        self.state_out = nn.Linear(in_features=self.embed_dim, out_features=3, bias=False)\n",
    "        \n",
    "        initializer_cfg = None\n",
    "        for layer in self.m1:\n",
    "            layer.apply(partial(_init_weights, n_layer=nb, **(initializer_cfg if initializer_cfg is not None else {})))\n",
    "        for layer in self.leftm:\n",
    "            layer.apply(partial(_init_weights, n_layer=no, **(initializer_cfg if initializer_cfg is not None else {})))\n",
    "        for layer in self.rightm:\n",
    "            layer.apply(partial(_init_weights, n_layer=ns, **(initializer_cfg if initializer_cfg is not None else {})))\n",
    "        \n",
    "            \n",
    "\n",
    "    def forward(self, x): # x is of shape (B, L, 5) (Batchsize, sequence length, dimension)\n",
    "        x = self.proj(x)\n",
    "        hidden_states, residuals = x, None\n",
    "        for layer in self.m1:\n",
    "            hidden_states, residuals = layer(hidden_states, residuals)\n",
    "        \n",
    "        left_hidden_states, left_residuals = hidden_states, residuals\n",
    "        right_hidden_states, right_residuals = hidden_states, residuals\n",
    "\n",
    "        for layer in self.leftm:\n",
    "            left_hidden_states, left_residuals = layer(left_hidden_states, left_residuals)\n",
    "        for layer in self.rightm:\n",
    "            right_hidden_states, right_residuals = layer(right_hidden_states, right_residuals)\n",
    "        \n",
    "        offset_out = self.offset_out(left_hidden_states)\n",
    "        state_out = self.state_out(right_hidden_states)\n",
    "        return offset_out, state_out\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, input_seq):\n",
    "        prev = input_seq\n",
    "        \n",
    "        i = 0\n",
    "        while i < 10:\n",
    "            offset, state = self.forward(prev)\n",
    "            pred = torch.cat((offset, state), dim=-1)\n",
    "            next_seg = pred[:, -1:, :]\n",
    "            prev = torch.cat((prev, next_seg), dim=1)\n",
    "            i += 1\n",
    "        \n",
    "        return prev\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04832976746641415\n"
     ]
    }
   ],
   "source": [
    "log_interval = 10\n",
    "epochs = 5\n",
    "\n",
    "batches = len(train)\n",
    "\n",
    "model = customModel(nb=4, no=2, ns=2, embed_dim=32, state_hidden=128).to(\"cuda\")\n",
    "\n",
    "offset_crit = nn.MSELoss(reduction='none')\n",
    "state_crit = nn.CrossEntropyLoss(ignore_index=2)\n",
    "\n",
    "#optimizer = torch.optim.RAdam(model.parameters(), lr=5e-4)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "print(0.05 * math.sqrt(batch_size / (batches * epochs)))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr=1e-4, betas=(0.99, 0.95), eps=1e-4,\n",
    "                              weight_decay=0.05 * math.sqrt(batch_size / (batches * epochs)))   \n",
    "# scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "#                                             num_warmup_steps=batches * warmup_ratio,\n",
    "#                                             num_training_steps=batches)\n",
    "\n",
    "writer = SummaryWriter('./logs')\n",
    "\n",
    "def train_model(model, data_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    size = len(data_loader)\n",
    "    \n",
    "    # Total Losses\n",
    "    total_loss = 0\n",
    "    total_offset_loss = 0\n",
    "    total_state_loss = 0\n",
    "    \n",
    "    # Running Losses\n",
    "    running_loss = 0\n",
    "    running_offset_loss = 0\n",
    "    running_state_loss = 0\n",
    "    \n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "    \n",
    "    running_mse = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, data in enumerate(tqdm(data_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs, targets = data\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "        targets = targets.to(\"cuda\")\n",
    "        \n",
    "        offsets, states = model(inputs)\n",
    "        \n",
    "        # Split Target\n",
    "        offset_target = targets[:, :, :2]\n",
    "        state_target = targets[:, :, 2:].argmax(dim=-1)\n",
    "        no_pad_mask = state_target != 2\n",
    "        \n",
    "        # Masked MSE Loss for offset\n",
    "        offset_loss = offset_crit(offsets, offset_target)\n",
    "        offset_loss_mask = offset_loss * no_pad_mask.unsqueeze(-1).float()\n",
    "        \n",
    "        offset_loss = offset_loss_mask.sum() / no_pad_mask.sum()\n",
    "        \n",
    "        # Cross Entropy Loss for State\n",
    "        state_loss = state_crit(states.transpose(1, 2), state_target)\n",
    "        loss = offset_loss + state_loss\n",
    "        \n",
    "        # Calculate other metrics (accuracy)\n",
    "        with torch.no_grad():\n",
    "            states_softmax = torch.nn.functional.softmax(states, dim=-1)\n",
    "            states_pred = states_softmax.argmax(dim=-1)\n",
    "            \n",
    "            no_pad_mask = state_target.flatten() != 2\n",
    "\n",
    "            running_correct += (states_pred.flatten()[no_pad_mask] == state_target.flatten()[no_pad_mask]).sum().item()\n",
    "            running_total += states_pred.flatten()[no_pad_mask].size().numel()\n",
    "            \n",
    "            flat_offsets_pred = offsets.reshape(-1, 2)[no_pad_mask, :]\n",
    "            flat_offset_target = targets[:, :, :2].reshape(-1, 2)[no_pad_mask, :]\n",
    "            running_mse += nn.functional.mse_loss(flat_offsets_pred, flat_offset_target)\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        # Gradient Clipping\n",
    "        for name, param in model.named_parameters():\n",
    "            torch.nn.utils.clip_grad_norm_(param, max_norm=1.0)\n",
    "        \n",
    "        # Optimizer Steps\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_offset_loss += offset_loss.item()\n",
    "        running_state_loss += state_loss.item()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_offset_loss += offset_loss.item()\n",
    "        total_state_loss += state_loss.item()\n",
    "        \n",
    "        # Print speed, losses, and accuracy every 25 batchs\n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = running_loss / log_interval\n",
    "            cur_offset_loss = running_offset_loss / log_interval\n",
    "            cur_state_loss = running_state_loss / log_interval\n",
    "            cur_accuracy = running_correct / running_total\n",
    "            cur_mse = running_mse / log_interval\n",
    "            tqdm.write(f'{prefix}| epoch {(epoch+1):3d} | {i:5d}/{size:5d} batches '\n",
    "                  f'| ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'offset_loss {cur_offset_loss:5.2f} | state_loss {cur_state_loss:5.4f} | '\n",
    "                  f'accuracy {cur_accuracy:5.4f} | mse {cur_mse:5.2f}')\n",
    "            time.sleep(0)\n",
    "            running_loss = 0\n",
    "            running_offset_loss = 0\n",
    "            running_state_loss = 0\n",
    "            running_correct = 0\n",
    "            running_total = 0\n",
    "            running_mse = 0\n",
    "            start_time = time.time()\n",
    "    \n",
    "    return total_loss / size, total_offset_loss / size, total_state_loss / size\n",
    "        \n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    size = len(data_loader)\n",
    "    \n",
    "    # Running Losses\n",
    "    running_loss = 0\n",
    "    running_offset_loss = 0\n",
    "    running_state_loss = 0\n",
    "    \n",
    "    # Correct\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader):\n",
    "            inputs, targets = data\n",
    "            inputs = inputs.to(\"cuda\")\n",
    "            targets = targets.to(\"cuda\")\n",
    "            \n",
    "            offsets, states = model(inputs)\n",
    "            \n",
    "            # Masked MSE Loss\n",
    "            offset_target = targets[:, :, :2]\n",
    "            state_target = targets[:, :, 2:].argmax(dim=-1)\n",
    "            no_pad_mask = state_target != 2\n",
    "            \n",
    "            offset_loss = offset_crit(offsets, offset_target)\n",
    "            offset_loss_mask = offset_loss * no_pad_mask.unsqueeze(-1).float()\n",
    "            offset_loss = offset_loss_mask.sum() / no_pad_mask.sum()\n",
    "            \n",
    "            state_loss = state_crit(states.transpose(1, 2), state_target)\n",
    "            loss = offset_loss + state_loss\n",
    "            \n",
    "            # Accuracy Calculation\n",
    "            states_softmax = torch.nn.functional.softmax(states, dim=-1)\n",
    "            states_pred = states_softmax.argmax(dim=-1)\n",
    "            \n",
    "            no_pad_mask = no_pad_mask.flatten()\n",
    "            \n",
    "            correct += (states_pred.flatten()[no_pad_mask] == state_target.flatten()[no_pad_mask]).sum().item()\n",
    "            total_samples += states_pred.flatten()[no_pad_mask].size().numel()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_offset_loss += offset_loss.item()\n",
    "            running_state_loss += state_loss.item()\n",
    "    \n",
    "    return running_loss / size, running_offset_loss / size, running_state_loss / size, correct / total_samples\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb06c8c2eec54e4996d01e311ac60e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   270/  274 batches | ms/batch 99.50 | offset_loss 10756.93 | state_loss 0.9578 | accuracy 0.8107 | mse 5378.463\n",
      "Training: Epoch: 1, offset_loss: 10815.6281, state_loss: 1.0415\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b38a83b4e2447e6814bf4c630901e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Epoch: 1, offset_loss: 10983.63, state_loss: 0.9482, accuracy: 0.8136\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_offset_loss, train_state_loss = train_model(model, train, optimizer, epoch)\n",
    "    print(f\"Training: Epoch: {epoch+1}, offset_loss: {train_offset_loss:5.4f}, state_loss: {train_state_loss:5.4f}\")\n",
    "    writer.add_scalar(\"Train/Loss/Epoch\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Train/Offset_Loss/Epoch\", train_offset_loss, epoch)\n",
    "    writer.add_scalar(\"Train/State_Loss/Epoch\", train_state_loss, epoch)\n",
    "    \n",
    "    val_loss, val_offset_loss, val_state_loss, val_accuracy = evaluate_model(model, val)\n",
    "    print(f\"Validation: Epoch: {epoch+1}, offset_loss: {val_offset_loss:5.2f}, state_loss: {val_state_loss:5.4f}, accuracy: {val_accuracy:5.4f}\")\n",
    "    writer.add_scalar(\"Train/Loss/Epoch\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Train/Offset_Loss/Epoch\", val_offset_loss, epoch)\n",
    "    writer.add_scalar(\"Train/State_Loss/Epoch\", val_state_loss, epoch)\n",
    "    \n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d2a71a51ae45fd93d2f6496b0ae8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: offset_loss: 10632.34, state_loss: 0.9628, accuracy: 0.8254\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_offset_loss, test_state_loss, test_accuracy = evaluate_model(model, test)\n",
    "print(f\"Test: offset_loss: {test_offset_loss:5.2f}, state_loss: {test_state_loss:5.4f}, accuracy: {test_accuracy:5.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./saved/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = customModel(nb=4, no=2, ns=2, embed_dim=32, state_hidden=128).to(\"cuda\")\n",
    "loaded_model.load('./saved/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80e5d822cdd4aa0ab751ae55587c8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: offset_loss: 10612.51, state_loss: 0.9628, accuracy: 0.8254\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_offset_loss, test_state_loss, test_accuracy = evaluate_model(loaded_model, test)\n",
    "print(f\"Test: offset_loss: {test_offset_loss:5.2f}, state_loss: {test_state_loss:5.4f}, accuracy: {test_accuracy:5.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 5])\n",
      "tensor([[[ 1.6000e+01, -5.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.9000e+01, -1.2000e+01,  1.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 4.6390e-02, -3.8057e-01,  2.8873e-01, -8.5750e-02, -1.7887e-01],\n",
      "         [ 3.2328e-02, -3.8309e-01,  6.6125e-02,  1.2527e-02, -1.7771e-02],\n",
      "         [-3.0285e-02, -7.4831e-01,  6.4276e-02, -5.0272e-03, -3.5066e-02],\n",
      "         [-6.7661e-02, -9.0558e-01,  9.7885e-02, -2.4203e-02, -6.5083e-02],\n",
      "         [-4.4013e-02, -7.8273e-01,  1.1584e-01, -1.1298e-02, -8.4086e-02],\n",
      "         [-5.8402e-02, -7.2081e-01,  1.2373e-01, -9.4407e-03, -8.1001e-02],\n",
      "         [-8.3008e-02, -7.2284e-01,  1.2082e-01, -8.5030e-03, -8.2599e-02],\n",
      "         [-1.0626e-01, -7.5436e-01,  1.2234e-01, -8.5693e-03, -8.5260e-02],\n",
      "         [-1.2505e-01, -7.7243e-01,  1.2460e-01, -8.4539e-03, -8.8489e-02],\n",
      "         [-1.3894e-01, -7.7977e-01,  1.2763e-01, -8.0513e-03, -9.1408e-02]]],\n",
      "       device='cuda:0')\n",
      "tensor([[  16.,   -5.,    1.,    0.,    0.],\n",
      "        [  19.,  -12.,    1.,    0.,    0.],\n",
      "        [  80.,  -56.,    1.,    0.,    0.],\n",
      "        [  22.,  -33.,    1.,    0.,    0.],\n",
      "        [  11.,  -29.,    1.,    0.,    0.],\n",
      "        [  17.,  -66.,    1.,    0.,    0.],\n",
      "        [   5.,  -38.,    1.,    0.,    0.],\n",
      "        [   2., -394.,    1.,    0.,    0.],\n",
      "        [   8.,  -37.,    1.,    0.,    0.],\n",
      "        [  13.,  -26.,    1.,    0.,    0.]])\n"
     ]
    }
   ],
   "source": [
    "i = (train_dataset[0][0][:2]).unsqueeze(0)\n",
    "print(i.shape)\n",
    "print(model.generate(i.to(\"cuda\")))\n",
    "print(train_dataset[0][0][:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drawenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

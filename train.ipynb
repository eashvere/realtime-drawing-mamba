{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 00:47:50.622761: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-06 00:47:51.233747: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from functools import partial\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.utils import _term_move_up\n",
    "\n",
    "prefix = _term_move_up() + '\\r'\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# from mamba import Mamba, MambaConfig\n",
    "from mamba_ssm import Mamba\n",
    "# from mamba_ssm.modules.mamba_simple import Block\n",
    "from mamba_ssm.models.mixer_seq_simple import create_block, _init_weights\n",
    "\n",
    "import transformers\n",
    "\n",
    "from data import DrawingDataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce GTX 1080\n",
      "_CudaDeviceProperties(name='NVIDIA GeForce GTX 1080', major=6, minor=1, total_memory=8112MB, multi_processor_count=20)\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())\n",
    "print(torch.cuda.get_device_properties(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 25/345 [00:10<02:14,  2.38it/s]\n",
      "  7%|▋         | 25/345 [00:00<00:04, 65.82it/s]\n",
      "  7%|▋         | 25/345 [00:00<00:04, 66.10it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DrawingDataset(data_path=\"./data\", split=\"train\", max_length=100)\n",
    "val_dataset = DrawingDataset(data_path=\"./data\", split=\"valid\", max_length=100)\n",
    "test_dataset = DrawingDataset(data_path=\"./data\", split=\"test\", max_length=100)\n",
    "\n",
    "train = DataLoader(dataset=train_dataset, batch_size=256, shuffle=True)\n",
    "val = DataLoader(dataset=val_dataset, batch_size=256, shuffle=True)\n",
    "test = DataLoader(dataset=test_dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class mambaBlock(nn.Module):\n",
    "#     def __init__(self, d_model, d_state, d_conv, expand, n_layers):\n",
    "#         super(mambaBlock, self).__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.d_state = d_state\n",
    "#         self.d_conv = d_conv\n",
    "#         self.expand = expand\n",
    "#         self.n_layers = n_layers\n",
    "#         self.layers = []\n",
    "\n",
    "#         self.layers = nn.ModuleList([Mamba(d_model, d_state, d_conv, expand) for _ in range(n_layers)])\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x)\n",
    "#         return x\n",
    "\n",
    "class customModel(nn.Module):\n",
    "    def __init__(self, nb, no, ns, embed_dim, state_hidden):\n",
    "        super(customModel, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.state_hidden = state_hidden\n",
    "        self.proj = nn.Linear(in_features=5, out_features=self.embed_dim)\n",
    "        \n",
    "        self.m1 = nn.ModuleList([create_block(self.embed_dim , device='cuda', layer_idx=f'm{i}') for i in range(nb)])\n",
    "        self.leftm = nn.ModuleList([create_block(self.embed_dim , device='cuda', layer_idx=f'l{i}') for i in range(no)])\n",
    "        self.rightm = nn.ModuleList([create_block(self.embed_dim , device='cuda', layer_idx=f'r{i}') for i in range(ns)])\n",
    "        \n",
    "        self.offset_out = nn.Linear(in_features=self.embed_dim, out_features=2)\n",
    "        self.state_out = nn.Linear(in_features=self.embed_dim, out_features=3)\n",
    "        \n",
    "        initializer_cfg = None\n",
    "        for layer in self.m1:\n",
    "            layer.apply(partial(_init_weights, n_layer=nb, **(initializer_cfg if initializer_cfg is not None else {})))\n",
    "        for layer in self.leftm:\n",
    "            layer.apply(partial(_init_weights, n_layer=no, **(initializer_cfg if initializer_cfg is not None else {})))\n",
    "        for layer in self.rightm:\n",
    "            layer.apply(partial(_init_weights, n_layer=ns, **(initializer_cfg if initializer_cfg is not None else {})))\n",
    "        \n",
    "            \n",
    "\n",
    "    def forward(self, x): # x is of shape (B, L, 5) (Batchsize, sequence length, dimension)\n",
    "        x = self.proj(x)\n",
    "        hidden_states, residuals = x, None\n",
    "        for layer in self.m1:\n",
    "            hidden_states, residuals = layer(hidden_states, residuals)\n",
    "        \n",
    "        left_hidden_states, left_residuals = hidden_states, residuals\n",
    "        right_hidden_states, right_residuals = hidden_states, residuals\n",
    "\n",
    "        for layer in self.leftm:\n",
    "            left_hidden_states, left_residuals = layer(left_hidden_states, left_residuals)\n",
    "        for layer in self.rightm:\n",
    "            right_hidden_states, right_residuals = layer(right_hidden_states, right_residuals)\n",
    "        \n",
    "        offset_out = self.offset_out(left_hidden_states)\n",
    "        state_out = self.state_out(right_hidden_states)\n",
    "        return offset_out, state_out\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "model = customModel(nb=4, no=2, ns=2, embed_dim=32, state_hidden=128).to(\"cuda\")\n",
    "\n",
    "offset_crit = nn.MSELoss(reduction='none')\n",
    "state_crit = nn.CrossEntropyLoss(ignore_index=2)\n",
    "\n",
    "optimizer = torch.optim.RAdam(model.parameters(), lr=5e-4)\n",
    "\n",
    "writer = SummaryWriter('./logs')\n",
    "\n",
    "log_interval = 25\n",
    "epochs = 10\n",
    "\n",
    "def train_model(model, data_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    size = len(data_loader)\n",
    "    \n",
    "    # Total Losses\n",
    "    total_loss = 0\n",
    "    total_offset_loss = 0\n",
    "    total_state_loss = 0\n",
    "    \n",
    "    # Running Losses\n",
    "    running_loss = 0\n",
    "    running_offset_loss = 0\n",
    "    running_state_loss = 0\n",
    "    \n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "    \n",
    "    running_mse = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, data in enumerate(tqdm(data_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs, targets = data\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "        targets = targets.to(\"cuda\")\n",
    "        \n",
    "        offsets, states = model(inputs)\n",
    "        \n",
    "        # Split Target\n",
    "        offset_target = targets[:, :, :2]\n",
    "        state_target = targets[:, :, 2:].argmax(dim=-1)\n",
    "        no_pad_mask = state_target != 2\n",
    "        \n",
    "        # Masked MSE Loss for offset\n",
    "        offset_loss = offset_crit(offsets, offset_target)\n",
    "        offset_loss_mask = offset_loss * no_pad_mask.unsqueeze(-1).float()\n",
    "        \n",
    "        offset_loss = offset_loss_mask.sum() / no_pad_mask.sum()\n",
    "        \n",
    "        # Cross Entropy Loss for State\n",
    "        state_loss = state_crit(states.transpose(1, 2), state_target)\n",
    "        loss = offset_loss + state_loss\n",
    "        \n",
    "        # Calculate other metrics (accuracy)\n",
    "        with torch.no_grad():\n",
    "            states_softmax = torch.nn.functional.softmax(states, dim=-1)\n",
    "            states_pred = states_softmax.argmax(dim=-1)\n",
    "            \n",
    "            no_pad_mask = state_target.flatten() != 2\n",
    "\n",
    "            running_correct += (states_pred.flatten()[no_pad_mask] == state_target.flatten()[no_pad_mask]).sum().item()\n",
    "            running_total += states_pred.flatten()[no_pad_mask].size().numel()\n",
    "            \n",
    "            flat_offsets_pred = offsets.reshape(-1, 2)[no_pad_mask, :]\n",
    "            flat_offset_target = targets[:, :, :2].reshape(-1, 2)[no_pad_mask, :]\n",
    "            running_mse += nn.functional.mse_loss(flat_offsets_pred, flat_offset_target)\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        # Gradient Clipping\n",
    "        for name, param in model.named_parameters():\n",
    "            torch.nn.utils.clip_grad_norm_(param, max_norm=1.0)\n",
    "        \n",
    "        # Optimizer Steps\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_offset_loss += offset_loss.item()\n",
    "        running_state_loss += state_loss.item()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_offset_loss += offset_loss.item()\n",
    "        total_state_loss += state_loss.item()\n",
    "        \n",
    "        # Print speed, losses, and accuracy every 25 batchs\n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = running_loss / log_interval\n",
    "            cur_offset_loss = running_offset_loss / log_interval\n",
    "            cur_state_loss = running_state_loss / log_interval\n",
    "            cur_accuracy = running_correct / running_total\n",
    "            cur_mse = running_mse / log_interval\n",
    "            tqdm.write(f'{prefix}| epoch {(epoch+1):3d} | {i:5d}/{size:5d} batches '\n",
    "                  f'| ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'offset_loss {cur_offset_loss:5.2f} | state_loss {cur_state_loss:5.4f} | '\n",
    "                  f'accuracy {cur_accuracy:5.4f} | mse {cur_mse:5.2f}')\n",
    "            time.sleep(0)\n",
    "            running_loss = 0\n",
    "            running_offset_loss = 0\n",
    "            running_state_loss = 0\n",
    "            running_correct = 0\n",
    "            running_total = 0\n",
    "            running_mse = 0\n",
    "            start_time = time.time()\n",
    "    \n",
    "    return total_loss / size, total_offset_loss / size, total_state_loss / size\n",
    "        \n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    size = len(data_loader)\n",
    "    \n",
    "    # Running Losses\n",
    "    running_loss = 0\n",
    "    running_offset_loss = 0\n",
    "    running_state_loss = 0\n",
    "    \n",
    "    # Correct\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(data_loader):\n",
    "            inputs, targets = data\n",
    "            inputs = inputs.to(\"cuda\")\n",
    "            targets = targets.to(\"cuda\")\n",
    "            \n",
    "            offsets, states = model(inputs)\n",
    "            \n",
    "            # Masked MSE Loss\n",
    "            offset_target = targets[:, :, :2]\n",
    "            state_target = targets[:, :, 2:].argmax(dim=-1)\n",
    "            no_pad_mask = state_target != 2\n",
    "            \n",
    "            offset_loss = offset_crit(offsets, offset_target)\n",
    "            offset_loss_mask = offset_loss * no_pad_mask.unsqueeze(-1).float()\n",
    "            offset_loss = offset_loss_mask.sum() / no_pad_mask.sum()\n",
    "            \n",
    "            state_loss = state_crit(states.transpose(1, 2), state_target)\n",
    "            loss = offset_loss + state_loss\n",
    "            \n",
    "            # Accuracy Calculation\n",
    "            states_softmax = torch.nn.functional.softmax(states, dim=-1)\n",
    "            states_pred = states_softmax.argmax(dim=-1)\n",
    "            \n",
    "            no_pad_mask = no_pad_mask.flatten()\n",
    "            \n",
    "            correct += (states_pred.flatten()[no_pad_mask] == state_target.flatten()[no_pad_mask]).sum().item()\n",
    "            total_samples += states_pred.flatten()[no_pad_mask].size().numel()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_offset_loss += offset_loss.item()\n",
    "            running_state_loss += state_loss.item()\n",
    "    \n",
    "    return running_loss / size, running_offset_loss / size, running_state_loss / size, correct / total_samples\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4f92c1d0104bdd9e662506eb3a7277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6462 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   300/ 6462 batches | ms/batch 102.05 | offset_loss 6399.16 | state_loss 0.3810 | accuracy 0.8949 | mse 3199.58\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m----> 3\u001b[0m     train_loss, train_offset_loss, train_state_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining: Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, offset_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_offset_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, state_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_state_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain/Loss/Epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_loss, epoch)\n",
      "Cell \u001b[0;32mIn[5], line 45\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, data_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     42\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m offsets, states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Split Target\u001b[39;00m\n\u001b[1;32m     48\u001b[0m offset_target \u001b[38;5;241m=\u001b[39m targets[:, :, :\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 58\u001b[0m, in \u001b[0;36mcustomModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m     right_hidden_states, right_residuals \u001b[38;5;241m=\u001b[39m layer(right_hidden_states, right_residuals)\n\u001b[1;32m     57\u001b[0m offset_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset_out(left_hidden_states)\n\u001b[0;32m---> 58\u001b[0m state_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright_hidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m offset_out, state_out\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.11/site-packages/torch/fx/traceback.py:68\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.11/traceback.py:41\u001b[0m, in \u001b[0;36mformat_list\u001b[0;34m(extracted_list)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_list\u001b[39m(extracted_list):\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format a list of tuples or FrameSummary objects for printing.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    Given a list of tuples or FrameSummary objects as returned by\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    whose source text line is not None.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextracted_list\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.11/traceback.py:538\u001b[0m, in \u001b[0;36mStackSummary.format\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    536\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame_summary \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 538\u001b[0m     formatted_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_frame_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_summary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m formatted_frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.11/traceback.py:466\u001b[0m, in \u001b[0;36mStackSummary.format_frame_summary\u001b[0;34m(self, frame_summary)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame_summary\u001b[38;5;241m.\u001b[39mline:\n\u001b[1;32m    465\u001b[0m     stripped_line \u001b[38;5;241m=\u001b[39m frame_summary\u001b[38;5;241m.\u001b[39mline\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m--> 466\u001b[0m     row\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstripped_line\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    468\u001b[0m     line \u001b[38;5;241m=\u001b[39m frame_summary\u001b[38;5;241m.\u001b[39m_original_line\n\u001b[1;32m    469\u001b[0m     orig_line_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(line)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_offset_loss, train_state_loss = train_model(model, train, optimizer, epoch)\n",
    "    print(f\"Training: Epoch: {epoch+1}, Loss: {train_loss}, offset_loss: {train_offset_loss}, state_loss: {train_state_loss}\")\n",
    "    writer.add_scalar(\"Train/Loss/Epoch\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Train/Offset_Loss/Epoch\", train_offset_loss, epoch)\n",
    "    writer.add_scalar(\"Train/State_Loss/Epoch\", train_state_loss, epoch)\n",
    "    \n",
    "    val_loss, val_offset_loss, val_state_loss = evaluate_model(model, val)\n",
    "    print(f\"Validation: Epoch: {epoch+1}, Loss: {val_loss}, offset_loss: {val_offset_loss}, state_loss: {val_state_loss}\")\n",
    "    writer.add_scalar(\"Train/Loss/Epoch\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Train/Offset_Loss/Epoch\", val_offset_loss, epoch)\n",
    "    writer.add_scalar(\"Train/State_Loss/Epoch\", val_state_loss, epoch)\n",
    "    \n",
    "    #optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drawenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
